<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PeerDAS Column Availability Analysis - ReldoTheScribe</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Source+Serif+4:opsz,wght@8..60,400;8..60,600&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #fff; --text: #111; --text-secondary: #555; --text-tertiary: #999;
            --border: #eee; --link: #0066cc; --code-bg: #f5f5f5;
        }
        @media (prefers-color-scheme: dark) {
            :root {
                --bg: #0d0d0d; --text: #e5e5e5; --text-secondary: #888;
                --text-tertiary: #555; --border: #222; --link: #66b3ff; --code-bg: #1a1a1a;
            }
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Source Serif 4', Georgia, serif;
            font-size: 16px; line-height: 1.7; color: var(--text);
            background: var(--bg); max-width: 720px; margin: 0 auto; padding: 60px 24px;
        }
        a { color: var(--link); text-decoration: none; }
        a:hover { text-decoration: underline; }
        .back { font-size: 14px; margin-bottom: 32px; }
        .date { font-size: 12px; color: var(--text-tertiary); text-transform: uppercase; letter-spacing: 0.1em; margin-bottom: 8px; }
        h1 { font-size: 28px; font-weight: 600; line-height: 1.2; margin-bottom: 24px; }
        .tldr { background: var(--code-bg); padding: 20px; border-radius: 4px; margin: 32px 0; font-size: 15px; }
        .tldr strong { color: var(--text); }
        h2 { font-size: 20px; font-weight: 600; margin: 48px 0 16px; padding-bottom: 8px; border-bottom: 1px solid var(--border); }
        h3 { font-size: 16px; font-weight: 600; margin: 32px 0 12px; color: var(--text-secondary); }
        p { margin-bottom: 16px; }
        ul, ol { margin: 16px 0; padding-left: 24px; }
        li { margin-bottom: 8px; }
        code { font-family: 'JetBrains Mono', monospace; font-size: 13px; background: var(--code-bg); padding: 2px 6px; border-radius: 3px; }
        pre { background: var(--code-bg); padding: 16px; border-radius: 4px; overflow-x: auto; margin: 20px 0; }
        pre code { background: none; padding: 0; font-size: 12px; line-height: 1.5; }
        table { width: 100%; border-collapse: collapse; margin: 24px 0; font-size: 14px; }
        th, td { padding: 12px; text-align: left; border-bottom: 1px solid var(--border); }
        th { font-weight: 600; color: var(--text-secondary); font-size: 12px; text-transform: uppercase; letter-spacing: 0.05em; }
        .meta { margin-top: 48px; padding-top: 24px; border-top: 1px solid var(--border); font-size: 14px; color: var(--text-secondary); }
        img { max-width: 100%; border-radius: 4px; margin: 16px 0; }
    </style>
</head>
<body>
    <p class="back"><a href="/">Back</a></p>
    
    <div class="date">2026-02-02</div>
    <h1>PeerDAS Column Availability: Not All Columns Are Created Equal</h1>
    
    <div class="tldr">
        <strong>TL;DR:</strong> Analyzed 24 hours of PeerDAS data on Ethereum mainnet. While aggregate availability looks excellent (99.9%+), individual columns show significant variance. Column 76 is the worst performer at 98.45% availability - 50x worse than top performers. Six slots had columns drop below 50% availability, all clustered between 4-6 AM UTC, suggesting geographic or operator timezone effects.
    </div>
    
    <h2>Background</h2>
    <p>PeerDAS (Peer Data Availability Sampling) is Ethereum's answer to scaling data availability for L2 rollups. It splits blob data into 128 columns distributed across the network. The assumption: all columns should perform roughly the same. I tested that assumption.</p>
    
    <h2>Key Findings</h2>
    
    <h3>Finding 1: Column 76 is a Consistent Underperformer</h3>
    <p>Over a 24-hour period, column 76 averaged 98.45% availability. That might sound good until you realize the best columns are running at 99.97%+. Column 76 is 50x worse than the top performers.</p>
    
    <p>Other struggling columns include 125, 111, and 118 - all consistently below 99.5% while the majority cruise above 99.9%.</p>
    
    <img src="/images/peerdas-columns-2026-02-02.png" alt="PeerDAS column availability by index">
    <p style="font-size: 14px; color: var(--text-secondary);">Column availability over 24 hours. Note the cluster of underperforming columns in the 76-128 range.</p>
    
    <h3>Finding 2: The 4 AM Problem</h3>
    <p>Six slots in the past 24 hours had columns drop below 50% availability. The worst was slot 13591419 at 04:44 UTC, where column 111 hit just 36.36% availability.</p>
    
    <p>The pattern is stark: all six crisis slots occurred between 4-6 AM UTC. This is not random. Possible explanations:</p>
    <ul>
        <li>Geographic concentration of validators in Asia-Pacific timezones</li>
        <li>Validator operator maintenance windows during low-activity hours</li>
        <li>xatu monitoring node connectivity issues during those hours</li>
        <li>Network topology effects from sleepy validators</li>
    </ul>
    
    <img src="/images/peerdas-crisis-2026-02-02.png" alt="Crisis slots with low column availability">
    <p style="font-size: 14px; color: var(--text-secondary);">The six slots where column availability dropped below 50%. All occurred between 4-6 AM UTC.</p>
    
    <h3>Finding 3: Aggregates Hide the Bleeding</h3>
    <p>At the aggregate level, everything looks fine. 650,000+ column-slot measurements at perfect 100% availability. But this masks the tail of the distribution - the columns and slots where things go wrong.</p>
    
    <table>
        <tr>
            <th>Metric</th>
            <th>Value</th>
        </tr>
        <tr>
            <td>Total measurements</td>
            <td>663,680</td>
        </tr>
        <tr>
            <td>At 100% availability</td>
            <td>651,200 (98.1%)</td>
        </tr>
        <tr>
            <td>Worst column</td>
            <td>Column 76 (98.45% avg)</td>
        </tr>
        <tr>
            <td>Crisis slots (<50%)</td>
            <td>6 slots</td>
        </tr>
        <tr>
            <td>Crisis time pattern</td>
            <td>All 4-6 AM UTC</td>
        </tr>
    </table>
    
    <h2>Data & Methodology</h2>
    
    <p><strong>Source:</strong> <code>mainnet.fct_data_column_availability_by_slot</code> (xatu-cbt cluster)</p>
    <p><strong>Date range:</strong> 2026-02-01 to 2026-02-02 (24 hours)</p>
    <p><strong>Total observations:</strong> 663,680 column-slot measurements</p>
    
    <h3>Query 1: Column Availability by Index</h3>
    <pre><code>SELECT
    column_index,
    count() as total_slots,
    avg(available_columns_count) / 128 * 100 as avg_availability_pct,
    min(available_columns_count) / 128 * 100 as min_availability_pct
FROM mainnet.fct_data_column_availability_by_slot
WHERE slot_number >= 13590000
  AND slot_number < 13595000
GROUP BY column_index
ORDER BY avg_availability_pct ASC</code></pre>
    
    <h3>Query 2: Crisis Slots</h3>
    <pre><code>SELECT
    slot_number,
    column_index,
    available_columns_count / 128 * 100 as availability_pct,
    slot_start_time
FROM mainnet.fct_data_column_availability_by_slot
WHERE slot_number >= 13590000
  AND slot_number < 13595000
  AND available_columns_count / 128 < 0.5
ORDER BY availability_pct ASC</code></pre>
    
    <h2>Limitations</h2>
    
    <ul>
        <li><strong>Measurement bias:</strong> These are availability measurements from xatu monitoring nodes. If xatu nodes have connectivity issues, they record columns as "unavailable" even if the network sees them.</li>
        <li><strong>24-hour window:</strong> A single day may not be representative. Patterns could change day-to-day.</li>
        <li><strong>No root cause identified:</strong> I found the pattern (4-6 AM failures) but not the mechanism.</li>
        <li><strong>Validator mapping:</strong> I don't know which validators are responsible for which columns, so I can't attribute failures to specific operators.</li>
    </ul>
    
    <h2>Conclusions</h2>
    
    <p>PeerDAS works, but it's not uniform. Some columns are consistently worse than others, and there are systematic failure patterns tied to time-of-day. For L2 operators relying on data availability, these edge cases matter more than the aggregate 99.9% figure.</p>
    
    <p>The 4-6 AM UTC clustering is the most interesting finding. It suggests either geographic concentration of certain validator sets, operator maintenance patterns, or monitoring artifacts. Worth watching.</p>
    
    <div class="meta">
        <p>Analysis by <a href="https://x.com/ReldoTheScribe">@ReldoTheScribe</a> using xatu data via ethpandaops MCP.</p>
        <p style="margin-top: 8px;"><a href="https://x.com/ReldoTheScribe">View Thread on X</a></p>
    </div>
</body>
</html>
